{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nprint(os.listdir(\"../input\"))\n",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['train_V2.csv', 'test_V2.csv', 'sample_submission_V2.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt  \n\nfrom timeit import default_timer as timer\nfrom sklearn import preprocessing\n\n!pip install ultimate\nfrom ultimate.mlp import MLP \n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint\n\nimport gc, sys\ngc.enable()",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Requirement already satisfied: ultimate in /opt/conda/lib/python3.6/site-packages (2.51.2)\r\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d653984bcc8e1f02e2b197390a9189177750d99"
      },
      "cell_type": "code",
      "source": "def state(message,start = True, time = 0):\n    if(start):\n        print(f'Working on {message} ... ')\n    else :\n        print(f'Working on {message} took ({round(time , 3)}) Sec \\n')",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b52a53e684fc5e66ef66f4ebbee631878cac9b1c"
      },
      "cell_type": "code",
      "source": "INPUT_DIR = \"../input/\"",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "087b73e7b1bb7bbfcf2f32e1be1ada3082da191a"
      },
      "cell_type": "code",
      "source": "def feature_engineering(is_train=True):\n    # When this function is used for the training data, load train_V2.csv :\n    if is_train: \n        print(\"processing train_V2.csv\")\n        df = pd.read_csv(INPUT_DIR + 'train_V2.csv')\n        \n        # Only take the samples with matches that have more than 1 player \n        # there are matches with no players or just one player ( those samples could affect our model badly) \n        df = df[df['maxPlace'] > 1]\n    \n    # When this function is used for the test data, load test_V2.csv :\n    else:\n        print(\"processing test_V2.csv\")\n        df = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n        \n    # Make a new feature indecating the total distance a player cut :\n    state('totalDistance')\n    s = timer()\n    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n    e = timer()\n    state('totalDistance', False, e - s)\n          \n\n    state('rankPoints')\n    s = timer()\n    # Process the 'rankPoints' feature by replacing any value of (-1) to be (0) :\n    df['rankPoints'] = np.where(df['rankPoints'] <= 0 ,0 , df['rankPoints'])\n    e = timer()                                  \n    state('rankPoints', False, e-s)\n    \n\n    target = 'winPlacePerc'\n    # Get a list of the features to be used\n    features = list(df.columns)\n    \n    # Remove some features from the features list :\n    features.remove(\"Id\")\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n    features.remove(\"matchDuration\")\n    features.remove(\"matchType\")\n    \n    y = None\n    \n    # If we are processing the training data, process the target\n    # (group the data by the match and the group then take the mean of the target) \n    if is_train: \n        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n        # Remove the target from the features list :\n        features.remove(target)\n    \n    # Make new features indicating the mean of the features ( grouped by match and group ) :\n    print(\"get group mean feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n    # Put the new features into a rank form ( max value will have the highest rank)\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    \n    # If we are processing the training data let df_out = the grouped  'matchId' and 'groupId'\n    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n    # If we are processing the test data let df_out = 'matchId' and 'groupId' without grouping \n    else: df_out = df[['matchId','groupId']]\n    \n    # Merge agg and agg_rank (that we got before) with df_out :\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # Make new features indicating the max value of the features for each group ( grouped by match )\n    print(\"get group max feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n    # Put the new features into a rank form ( max value will have the highest rank)\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    # Merge the new (agg and agg_rank) with df_out :\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # Make new features indicating the minimum value of the features for each group ( grouped by match )\n    print(\"get group min feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n    # Put the new features into a rank form ( max value will have the highest rank)\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    # Merge the new (agg and agg_rank) with df_out :\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # Make new features indicating the number of players in each group ( grouped by match )\n    print(\"get group size feature\")\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n     \n    # Merge the group_size feature with df_out :\n    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n    \n    # Make new features indicating the mean value of each features for each match :\n    print(\"get match mean feature\")\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    \n    # Merge the new agg with df_out :\n    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n    \n    # Make new features indicating the number of groups in each match :\n    print(\"get match size feature\")\n    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n    \n    # Merge the match_size feature with df_out :\n    df_out = df_out.merge(agg, how='left', on=['matchId'])\n    \n    # Drop matchId and groupId\n    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n    \n    # X is the output dataset (without the target) and y is the target :\n    X = np.array(df_out, dtype=np.float64)\n    \n    \n    del df, df_out, agg, agg_rank\n    gc.collect()\n\n    return X, y",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb5e4ecedd5fbf4a6a532be914d9a4eff240a30e"
      },
      "cell_type": "code",
      "source": "%%time\n# Process the training data :\nx_train, y = feature_engineering(True)\n# Scale the data to be in the range (-1 , 1)\nscaler = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=False).fit(x_train)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "processing train_V2.csv\nWorking on totalDistance ... \nWorking on totalDistance took (0.323) Sec \n\nWorking on rankPoints ... \nWorking on rankPoints took (0.059) Sec \n\nget group mean feature\nget group max feature\nget group min feature\nget group size feature\nget match mean feature\nget match size feature\nCPU times: user 4min 4s, sys: 54.5 s, total: 4min 58s\nWall time: 4min 58s\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e97b058014a72d9b27ab5c9de986c844559fc0dc"
      },
      "cell_type": "code",
      "source": "print(\"x_train\", x_train.shape, x_train.max(), x_train.min())\nscaler.transform(x_train)\nprint(\"x_train\", x_train.shape, x_train.max(), x_train.min())",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "x_train (2026744, 170) 41270.1 0.0\nx_train (2026744, 170) 1.0000000000000002 -1.0000000000000002\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6cd593374c10c7502c16657464021916ce31e55f"
      },
      "cell_type": "code",
      "source": "y = y * 2 - 1\nprint(\"y\", y.shape, y.max(), y.min())",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "y (2026744,) 1.0 -1.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "25cab8994eb076c1f56593cf6f34c13f494f22e9"
      },
      "cell_type": "code",
      "source": "%%time\n# create NN_model\nNN_model = Sequential()\nNN_model.add(Dense(x_train.shape[1],  input_dim = x_train.shape[1], activation='relu'))\nNN_model.add(Dense(136, activation='relu'))\nNN_model.add(Dense(136, activation='relu'))\nNN_model.add(Dense(136, activation='relu'))\nNN_model.add(Dense(136, activation='relu'))\n\n# output Layer\nNN_model.add(Dense(1, activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 170)               29070     \n_________________________________________________________________\ndense_2 (Dense)              (None, 136)               23256     \n_________________________________________________________________\ndense_3 (Dense)              (None, 136)               18632     \n_________________________________________________________________\ndense_4 (Dense)              (None, 136)               18632     \n_________________________________________________________________\ndense_5 (Dense)              (None, 136)               18632     \n_________________________________________________________________\ndense_6 (Dense)              (None, 1)                 137       \n=================================================================\nTotal params: 108,359\nTrainable params: 108,359\nNon-trainable params: 0\n_________________________________________________________________\nCPU times: user 136 ms, sys: 4 ms, total: 140 ms\nWall time: 173 ms\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "409aa1ed9270050f7d739698cb723eba28e9604d"
      },
      "cell_type": "code",
      "source": "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d64676a2826176522fff168fbc097d021cb7a1b0"
      },
      "cell_type": "code",
      "source": "all_mae_histories = []\nmae_history = history.history['val_mean_absolute_error']\nall_mae_histories.append(mae_history)\n%%time\nNN_model.fit(x=x_train, y=y, batch_size=1000,\n             epochs=30, verbose=1, callbacks=callbacks_list,\n             validation_split=0.15, validation_data=None, shuffle=True,\n             class_weight=None, sample_weight=None, initial_epoch=0,\n             steps_per_epoch=None, validation_steps=None)\ndel x_train, y\ngc.collect()",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1722732 samples, validate on 304012 samples\nEpoch 1/30\n1722732/1722732 [==============================] - 58s 33us/step - loss: 0.0761 - mean_absolute_error: 0.0761 - val_loss: 0.0691 - val_mean_absolute_error: 0.0691\n\nEpoch 00001: val_loss improved from inf to 0.06912, saving model to Weights-001--0.06912.hdf5\nEpoch 2/30\n1722732/1722732 [==============================] - 55s 32us/step - loss: 0.0658 - mean_absolute_error: 0.0658 - val_loss: 0.0672 - val_mean_absolute_error: 0.0672\n\nEpoch 00002: val_loss improved from 0.06912 to 0.06721, saving model to Weights-002--0.06721.hdf5\nEpoch 3/30\n1722732/1722732 [==============================] - 55s 32us/step - loss: 0.0640 - mean_absolute_error: 0.0640 - val_loss: 0.0618 - val_mean_absolute_error: 0.0618\n\nEpoch 00003: val_loss improved from 0.06721 to 0.06182, saving model to Weights-003--0.06182.hdf5\nEpoch 4/30\n1722732/1722732 [==============================] - 58s 34us/step - loss: 0.0628 - mean_absolute_error: 0.0628 - val_loss: 0.0610 - val_mean_absolute_error: 0.0610\n\nEpoch 00004: val_loss improved from 0.06182 to 0.06097, saving model to Weights-004--0.06097.hdf5\nEpoch 5/30\n1722732/1722732 [==============================] - 56s 32us/step - loss: 0.0623 - mean_absolute_error: 0.0623 - val_loss: 0.0647 - val_mean_absolute_error: 0.0647\n\nEpoch 00005: val_loss did not improve from 0.06097\nEpoch 6/30\n1722732/1722732 [==============================] - 56s 33us/step - loss: 0.0620 - mean_absolute_error: 0.0620 - val_loss: 0.0611 - val_mean_absolute_error: 0.0611\n\nEpoch 00006: val_loss did not improve from 0.06097\nEpoch 7/30\n1722732/1722732 [==============================] - 55s 32us/step - loss: 0.0617 - mean_absolute_error: 0.0617 - val_loss: 0.0623 - val_mean_absolute_error: 0.0623\n\nEpoch 00007: val_loss did not improve from 0.06097\nEpoch 8/30\n1722732/1722732 [==============================] - 56s 33us/step - loss: 0.0613 - mean_absolute_error: 0.0613 - val_loss: 0.0655 - val_mean_absolute_error: 0.0655\n\nEpoch 00008: val_loss did not improve from 0.06097\nEpoch 9/30\n1722732/1722732 [==============================] - 59s 34us/step - loss: 0.0612 - mean_absolute_error: 0.0612 - val_loss: 0.0601 - val_mean_absolute_error: 0.0601\n\nEpoch 00009: val_loss improved from 0.06097 to 0.06006, saving model to Weights-009--0.06006.hdf5\nEpoch 10/30\n1722732/1722732 [==============================] - 55s 32us/step - loss: 0.0609 - mean_absolute_error: 0.0609 - val_loss: 0.0633 - val_mean_absolute_error: 0.0633\n\nEpoch 00010: val_loss did not improve from 0.06006\nEpoch 11/30\n1722732/1722732 [==============================] - 57s 33us/step - loss: 0.0607 - mean_absolute_error: 0.0607 - val_loss: 0.0602 - val_mean_absolute_error: 0.0602\n\nEpoch 00011: val_loss did not improve from 0.06006\nEpoch 12/30\n1722732/1722732 [==============================] - 57s 33us/step - loss: 0.0604 - mean_absolute_error: 0.0604 - val_loss: 0.0602 - val_mean_absolute_error: 0.0602\n\nEpoch 00012: val_loss did not improve from 0.06006\nEpoch 13/30\n1722732/1722732 [==============================] - 55s 32us/step - loss: 0.0605 - mean_absolute_error: 0.0605 - val_loss: 0.0648 - val_mean_absolute_error: 0.0648\n\nEpoch 00013: val_loss did not improve from 0.06006\nEpoch 14/30\n1722732/1722732 [==============================] - 56s 32us/step - loss: 0.0601 - mean_absolute_error: 0.0601 - val_loss: 0.0600 - val_mean_absolute_error: 0.0600\n\nEpoch 00014: val_loss improved from 0.06006 to 0.05998, saving model to Weights-014--0.05998.hdf5\nEpoch 15/30\n1722732/1722732 [==============================] - 55s 32us/step - loss: 0.0601 - mean_absolute_error: 0.0601 - val_loss: 0.0589 - val_mean_absolute_error: 0.0589\n\nEpoch 00015: val_loss improved from 0.05998 to 0.05895, saving model to Weights-015--0.05895.hdf5\nEpoch 16/30\n1722732/1722732 [==============================] - 55s 32us/step - loss: 0.0599 - mean_absolute_error: 0.0599 - val_loss: 0.0602 - val_mean_absolute_error: 0.0602\n\nEpoch 00016: val_loss did not improve from 0.05895\nEpoch 17/30\n1722732/1722732 [==============================] - 57s 33us/step - loss: 0.0597 - mean_absolute_error: 0.0597 - val_loss: 0.0587 - val_mean_absolute_error: 0.0587\n\nEpoch 00017: val_loss improved from 0.05895 to 0.05874, saving model to Weights-017--0.05874.hdf5\nEpoch 18/30\n1722732/1722732 [==============================] - 55s 32us/step - loss: 0.0597 - mean_absolute_error: 0.0597 - val_loss: 0.0589 - val_mean_absolute_error: 0.0589\n\nEpoch 00018: val_loss did not improve from 0.05874\nEpoch 19/30\n1722732/1722732 [==============================] - 56s 32us/step - loss: 0.0595 - mean_absolute_error: 0.0595 - val_loss: 0.0614 - val_mean_absolute_error: 0.0614\n\nEpoch 00019: val_loss did not improve from 0.05874\nEpoch 20/30\n1722732/1722732 [==============================] - 57s 33us/step - loss: 0.0595 - mean_absolute_error: 0.0595 - val_loss: 0.0602 - val_mean_absolute_error: 0.0602\n\nEpoch 00020: val_loss did not improve from 0.05874\nEpoch 21/30\n1722732/1722732 [==============================] - 61s 35us/step - loss: 0.0594 - mean_absolute_error: 0.0594 - val_loss: 0.0597 - val_mean_absolute_error: 0.0597\n\nEpoch 00021: val_loss did not improve from 0.05874\nEpoch 22/30\n1722732/1722732 [==============================] - 58s 34us/step - loss: 0.0592 - mean_absolute_error: 0.0592 - val_loss: 0.0618 - val_mean_absolute_error: 0.0618\n\nEpoch 00022: val_loss did not improve from 0.05874\nEpoch 23/30\n1722732/1722732 [==============================] - 57s 33us/step - loss: 0.0592 - mean_absolute_error: 0.0592 - val_loss: 0.0597 - val_mean_absolute_error: 0.0597\n\nEpoch 00023: val_loss did not improve from 0.05874\nEpoch 24/30\n1722732/1722732 [==============================] - 59s 34us/step - loss: 0.0592 - mean_absolute_error: 0.0592 - val_loss: 0.0592 - val_mean_absolute_error: 0.0592\n\nEpoch 00024: val_loss did not improve from 0.05874\nEpoch 25/30\n1722732/1722732 [==============================] - 57s 33us/step - loss: 0.0590 - mean_absolute_error: 0.0590 - val_loss: 0.0583 - val_mean_absolute_error: 0.0583\n\nEpoch 00025: val_loss improved from 0.05874 to 0.05828, saving model to Weights-025--0.05828.hdf5\nEpoch 26/30\n1722732/1722732 [==============================] - 55s 32us/step - loss: 0.0590 - mean_absolute_error: 0.0590 - val_loss: 0.0582 - val_mean_absolute_error: 0.0582\n\nEpoch 00026: val_loss improved from 0.05828 to 0.05824, saving model to Weights-026--0.05824.hdf5\nEpoch 27/30\n1722732/1722732 [==============================] - 55s 32us/step - loss: 0.0590 - mean_absolute_error: 0.0590 - val_loss: 0.0605 - val_mean_absolute_error: 0.0605\n\nEpoch 00027: val_loss did not improve from 0.05824\nEpoch 28/30\n1722732/1722732 [==============================] - 56s 32us/step - loss: 0.0587 - mean_absolute_error: 0.0587 - val_loss: 0.0585 - val_mean_absolute_error: 0.0585\n\nEpoch 00028: val_loss did not improve from 0.05824\nEpoch 29/30\n1722732/1722732 [==============================] - 55s 32us/step - loss: 0.0588 - mean_absolute_error: 0.0588 - val_loss: 0.0590 - val_mean_absolute_error: 0.0590\n\nEpoch 00029: val_loss did not improve from 0.05824\nEpoch 30/30\n1722732/1722732 [==============================] - 58s 33us/step - loss: 0.0586 - mean_absolute_error: 0.0586 - val_loss: 0.0598 - val_mean_absolute_error: 0.0598\n\nEpoch 00030: val_loss did not improve from 0.05824\nCPU times: user 47min 34s, sys: 5min 47s, total: 53min 22s\nWall time: 28min 12s\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3bb0c878d5b9f289626f2a3814ae97698fb1ac54"
      },
      "cell_type": "code",
      "source": "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(200)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9bc03825e06da59301db604f84471d571bc8a332"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n\nplt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a8ff609270d6ece62a1f815c6376d9fee6671da"
      },
      "cell_type": "code",
      "source": "def plot_history(history):\n    # print(history.history.keys())\n\n    # 精度の履歴をプロット\n    plt.plot(history.history['mean_absolute_error'])\n    plt.plot(history.history['val_mean_absolute_error'])\n    plt.title('model accuracy')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.legend(['mean_absolute_error', 'val_mean_absolute_error'], loc='lower right')\n    plt.show()\n\n    # 損失の履歴をプロット\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['loss', 'val_loss'], loc='lower right')\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce20c5339af7051d810aa628778f25c1baad2ffd"
      },
      "cell_type": "code",
      "source": "plot_history(history)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "616c07802f719df6debdd81daa651b9834694480"
      },
      "cell_type": "code",
      "source": "x_test, _ = feature_engineering(False)\nscaler.transform(x_test)\nprint(\"x_test\", x_test.shape, x_test.max(), x_test.min())\nnp.clip(x_test, out=x_test, a_min=-1, a_max=1)\nprint(\"x_test\", x_test.shape, x_test.max(), x_test.min())",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "processing test_V2.csv\nWorking on totalDistance ... \nWorking on totalDistance took (0.014) Sec \n\nWorking on rankPoints ... \nWorking on rankPoints took (0.027) Sec \n\nget group mean feature\nget group max feature\nget group min feature\nget group size feature\nget match mean feature\nget match size feature\nx_test (1934174, 170) 2.8037403272580264 -1.0633484162895928\nx_test (1934174, 170) 1.0 -1.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e53391398b5305716707a0dff297c8a70b8a4e5f"
      },
      "cell_type": "code",
      "source": "%%time\npred = NN_model.predict(x_test)\ndel x_test\ngc.collect()",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "CPU times: user 1min 2s, sys: 9.96 s, total: 1min 12s\nWall time: 47.9 s\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "932b9592f7d8104d0dbbc03dd0e2be6f4a08d910"
      },
      "cell_type": "code",
      "source": "pred = pred.reshape(-1)\npred = (pred + 1) / 2",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a57f65a46b7268821949470863a73f2f356bca77"
      },
      "cell_type": "code",
      "source": "df_test = pd.read_csv(INPUT_DIR + 'test_V2.csv')",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9353de80b967ca282eccaabda141fa0ffeda1838"
      },
      "cell_type": "code",
      "source": "%%time\nprint(\"fix winPlacePerc\")\nfor i in range(len(df_test)):\n    winPlacePerc = pred[i]\n    maxPlace = int(df_test.iloc[i]['maxPlace'])\n    if maxPlace == 0:\n        winPlacePerc = 0.0\n    elif maxPlace == 1:\n        winPlacePerc = 1.0\n    else:\n        gap = 1.0 / (maxPlace - 1)\n        winPlacePerc = round(winPlacePerc / gap) * gap\n    \n    if winPlacePerc < 0: winPlacePerc = 0.0\n    if winPlacePerc > 1: winPlacePerc = 1.0    \n    pred[i] = winPlacePerc",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "fix winPlacePerc\nCPU times: user 6min 26s, sys: 0 ns, total: 6min 26s\nWall time: 6min 26s\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c2de784e2afa875e7226194d72e516082891c0b"
      },
      "cell_type": "code",
      "source": "df_test['winPlacePerc'] = pred",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f47368cb280647a26160a04a1160d123296258b4"
      },
      "cell_type": "code",
      "source": "submission = df_test[['Id', 'winPlacePerc']]\nsubmission.to_csv('submission.csv', index=False)",
      "execution_count": 19,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}